{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Download & save MiniLM\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model.save('chatbot/models/minilm')\n",
    "\n",
    "# Download & save MPNet\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "model.save('chatbot/models/mpnet')\n",
    "\n",
    "# Download & save T5-small\n",
    "t5 = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5.save_pretrained('chatbot/models/t5_model')\n",
    "tokenizer.save_pretrained('chatbot/models/t5_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_directory_tree_with_os_walk(starting_directory):\n",
    "    for root, directories, files in os.walk(starting_directory):\n",
    "        print(f\"Directory: {root}\")\n",
    "        for file in files:\n",
    "            print(f\"  File: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: .\n",
      "  File: AeroViz.py\n",
      "  File: foo.ipynb\n",
      "  File: icon.png\n",
      "  File: outstanding.txt\n",
      "  File: README.md\n",
      "  File: requirements.txt\n",
      "  File: runtime.txt\n",
      "  File: stylesheet.py\n",
      "Directory: .\\.streamlit\n",
      "  File: config.toml\n",
      "Directory: .\\chatbot\n",
      "  File: retrieval.py\n",
      "  File: summarizer.py\n",
      "  File: __init__.py\n",
      "Directory: .\\chatbot\\data\n",
      "  File: embedded_papers.json\n",
      "  File: faiss.index\n",
      "Directory: .\\chatbot\\models\n",
      "Directory: .\\chatbot\\models\\minilm\n",
      "  File: config.json\n",
      "  File: config_sentence_transformers.json\n",
      "  File: model.safetensors\n",
      "  File: modules.json\n",
      "  File: README.md\n",
      "  File: sentence_bert_config.json\n",
      "  File: special_tokens_map.json\n",
      "  File: tokenizer.json\n",
      "  File: tokenizer_config.json\n",
      "  File: vocab.txt\n",
      "Directory: .\\chatbot\\models\\minilm\\1_Pooling\n",
      "  File: config.json\n",
      "Directory: .\\chatbot\\models\\minilm\\2_Normalize\n",
      "Directory: .\\chatbot\\models\\mpnet\n",
      "  File: config.json\n",
      "  File: config_sentence_transformers.json\n",
      "  File: model.safetensors\n",
      "  File: modules.json\n",
      "  File: README.md\n",
      "  File: sentence_bert_config.json\n",
      "  File: special_tokens_map.json\n",
      "  File: tokenizer.json\n",
      "  File: tokenizer_config.json\n",
      "  File: vocab.txt\n",
      "Directory: .\\chatbot\\models\\mpnet\\1_Pooling\n",
      "  File: config.json\n",
      "Directory: .\\chatbot\\models\\mpnet\\2_Normalize\n",
      "Directory: .\\chatbot\\papers_raw\n",
      "Directory: .\\foo\n",
      "  File: 3_CBT2.py\n",
      "Directory: .\\icons\n",
      "  File: calculator.ico\n",
      "  File: icon.ico\n",
      "Directory: .\\modules\n",
      "  File: Blisks.py\n",
      "  File: CBT_Flutter.py\n",
      "Directory: .\\modules\\__pycache__\n",
      "  File: Blisks.cpython-312.pyc\n",
      "  File: Blisks.cpython-313.pyc\n",
      "  File: CBT_Flutter.cpython-313.pyc\n",
      "  File: stylesheet.cpython-313.pyc\n",
      "Directory: .\\pages\n",
      "  File: 1_CBT.py\n",
      "  File: 2_Blisks.py\n",
      "  File: 3_Assistant.py\n",
      "Directory: .\\__pycache__\n",
      "  File: stylesheet.cpython-313.pyc\n"
     ]
    }
   ],
   "source": [
    "list_directory_tree_with_os_walk('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayode\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing topic: blisks\n",
      "ðŸ“„ Converting Bladed Disks VKI Roque Coral.pdf...\n",
      "âœ… Saved text to chatbot/papers_raw\\blisks\\Bladed Disks VKI Roque Coral.txt\n",
      "âœ… Saved index for topic 'blisks' with 128 chunks.\n",
      "\n",
      "ðŸ” Processing topic: cbt_flutter\n",
      "ðŸ“„ Converting CBT Flutter Part 1 Notes VKI.pdf...\n",
      "âœ… Saved text to chatbot/papers_raw\\cbt_flutter\\CBT Flutter Part 1 Notes VKI.txt\n",
      "âœ… Saved index for topic 'cbt_flutter' with 91 chunks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# --- Constants --- #\n",
    "MODEL_PATH = \"chatbot/models/minilm\"\n",
    "RAW_BASE_DIR = \"chatbot/papers_raw\"\n",
    "OUT_BASE_DIR = \"chatbot/data\"\n",
    "\n",
    "# --- Helpers --- #\n",
    "def chunk_text(text, chunk_size=5):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [\" \".join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join(page.get_text() for page in doc)\n",
    "\n",
    "# --- Load Model --- #\n",
    "embedder = SentenceTransformer(MODEL_PATH)\n",
    "\n",
    "# --- Process Each Topic Folder --- #\n",
    "for topic in os.listdir(RAW_BASE_DIR):\n",
    "    topic_dir = os.path.join(RAW_BASE_DIR, topic)\n",
    "    if not os.path.isdir(topic_dir):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nðŸ” Processing topic: {topic}\")\n",
    "    doc_chunks = {}\n",
    "    all_embeddings = []\n",
    "    chunk_id = 0\n",
    "\n",
    "    # Convert PDFs to .txt\n",
    "    for file in os.listdir(topic_dir):\n",
    "        filepath = os.path.join(topic_dir, file)\n",
    "\n",
    "        if file.endswith(\".pdf\"):\n",
    "            print(f\"ðŸ“„ Converting {file}...\")\n",
    "            text = pdf_to_text(filepath)\n",
    "            txt_path = filepath.replace(\".pdf\", \".txt\")\n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "            print(f\"âœ… Saved text to {txt_path}\")\n",
    "\n",
    "    # Read all .txt files (original or converted)\n",
    "    for file in os.listdir(topic_dir):\n",
    "        if not file.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(topic_dir, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            chunks = chunk_text(text)\n",
    "            embeddings = embedder.encode(chunks)\n",
    "\n",
    "            for emb, chunk in zip(embeddings, chunks):\n",
    "                doc_chunks[str(chunk_id)] = chunk\n",
    "                all_embeddings.append(emb)\n",
    "                chunk_id += 1\n",
    "\n",
    "    # Save output\n",
    "    out_dir = os.path.join(OUT_BASE_DIR, topic)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"embedded_papers.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc_chunks, f)\n",
    "\n",
    "    index = faiss.IndexFlatL2(len(all_embeddings[0]))\n",
    "    index.add(np.array(all_embeddings).astype(\"float32\"))\n",
    "    faiss.write_index(index, os.path.join(out_dir, \"faiss.index\"))\n",
    "\n",
    "    print(f\"âœ… Saved index for topic '{topic}' with {chunk_id} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:59:22.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.463 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.463 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.464 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.467 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.467 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.468 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.468 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.572 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.574 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.574 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-28 17:59:22.575 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'chatbot/data/embedded_papers.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# --- Load FAISS index + chunk mapping --- #\u001b[39;00m\n\u001b[0;32m     31\u001b[0m st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading embedded documents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchatbot/data/embedded_papers.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     33\u001b[0m     doc_chunks \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     35\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mread_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/faiss.index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chatbot/data/embedded_papers.json'"
     ]
    }
   ],
   "source": [
    "# ðŸ“ project_root/\n",
    "# â”œâ”€â”€ app.py\n",
    "# â”œâ”€â”€ models/\n",
    "# â”‚   â”œâ”€â”€ minilm/  # from HuggingFace 'all-MiniLM-L6-v2'\n",
    "# â”‚   â””â”€â”€ mpnet/   # from HuggingFace 'all-mpnet-base-v2'\n",
    "# â”œâ”€â”€ data/\n",
    "# â”‚   â”œâ”€â”€ embedded_papers.json\n",
    "# â”‚   â””â”€â”€ papers_raw/  # optional: raw text versions of PDFs\n",
    "# â”œâ”€â”€ summarizer/\n",
    "# â”‚   â””â”€â”€ t5_model/  # T5-small model\n",
    "# â””â”€â”€ requirements.txt\n",
    "\n",
    "import streamlit as st\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# --- Sidebar Model Toggle --- #\n",
    "model_choice = st.sidebar.selectbox(\"Choose embedding model\", [\"MiniLM\", \"MPNet\"])\n",
    "model_path = f\"chatbot/models/{'minilm' if model_choice == 'MiniLM' else 'mpnet'}\"\n",
    "\n",
    "st.title(\"ðŸ“š Research Assistant Chatbot\")\n",
    "\n",
    "# --- Load Embedding Model --- #\n",
    "st.write(f\"Loading embedding model: {model_choice}\")\n",
    "embedder = SentenceTransformer(model_path)\n",
    "\n",
    "# --- Load FAISS index + chunk mapping --- #\n",
    "st.write(\"Loading embedded documents...\")\n",
    "with open(\"chatbot/data/embedded_papers.json\", \"r\") as f:\n",
    "    doc_chunks = json.load(f)\n",
    "\n",
    "index = faiss.read_index(\"data/faiss.index\")\n",
    "\n",
    "# --- Load T5 summarizer --- #\n",
    "summarizer_model = T5ForConditionalGeneration.from_pretrained(\"summarizer/t5_model\")\n",
    "summarizer_tokenizer = T5Tokenizer.from_pretrained(\"summarizer/t5_model\")\n",
    "\n",
    "# --- User Input --- #\n",
    "query = st.text_input(\"Ask a question about the papers:\")\n",
    "\n",
    "if query:\n",
    "    \n",
    "    query_vec = embedder.encode([query])\n",
    "    D, I = index.search(np.array(query_vec).astype(\"float32\"), k=3)\n",
    "\n",
    "    # Retrieve top chunks\n",
    "    relevant_chunks = [doc_chunks[str(i)] for i in I[0]]\n",
    "\n",
    "    # Concatenate for summarizer input\n",
    "    input_text = \" \".join(relevant_chunks)\n",
    "    input_ids = summarizer_tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    summary_ids = summarizer_model.generate(input_ids, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    output = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    st.subheader(\"Answer:\")\n",
    "    st.write(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "     ---------------------------------------- 8.1/8.1 MB 6.1 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
      "     -------------------------------------- 219.8/219.8 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 6.1 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "     ---------------------------------------- 72.0/72.0 kB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\ayode\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ayode\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ayode\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "     -------------------------------------- 111.1/111.1 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ayode\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ayode\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
